{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aya-se/advanced-machine-learning-2022/blob/main/aml2022_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z-I39fBi1MD"
      },
      "source": [
        "# Advanced Machine Learning (2022) Final Report Assignment\n",
        "\n",
        "Answer Questions 1 to 4 (either in Japanese or English). Submit a report in either PDF (.pdf) or JupyterNotebook (.ipynb) format.\n",
        "\n",
        "## Question 1 (50 points)\n",
        "\n",
        "Consider a convolutional neural network (CNN) that predicts a label $\\hat{y} \\in \\{0, 1\\}$ for a given sentence $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times T}$. Here, a sentence is represented by a matrix $\\boldsymbol{X} = (\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_T)$ consisting of a concatenation of $T$ word embeddings, $\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_T \\in \\mathbb{R}^d$, where $d$ is the size of word embeddings, and $T$ is the number of words in the sentence.\n",
        "\n",
        "These equations define the whole architecture of the CNN.\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y} &= \\begin{cases}\n",
        "1 & (0.5 < p) \\\\\n",
        "0 & (p \\leq 0.5)\n",
        "\\end{cases} \\\\\n",
        "p &= \\sigma(\\boldsymbol{v}^\\top \\boldsymbol{s}) \\\\\n",
        "\\boldsymbol{s} &= \\max(\\boldsymbol{c}_1, \\dots, \\boldsymbol{c}_{T-\\delta+1}) \\\\\n",
        "\\boldsymbol{c}_t &= {\\rm ReLU}(\\boldsymbol{W} \\boldsymbol{x}_{t:t+\\delta-1} + \\boldsymbol{b}) & (\\forall t \\in \\{1, \\dots, T-\\delta+1\\}) \\\\\n",
        "\\boldsymbol{x}_{t:t+\\delta-1} &= \\boldsymbol{x}_{t} \\oplus \\boldsymbol{x}_{t+1} \\oplus \\dots \\oplus \\boldsymbol{x}_{t+\\delta-1}\n",
        "\\end{align}\n",
        "\n",
        "Here:\n",
        "\n",
        "+ $\\boldsymbol{W} \\in \\mathbb{R}^{m \\times \\delta d}$, $\\boldsymbol{b} \\in \\mathbb{R}^m, \\boldsymbol{v} \\in \\mathbb{R}^m$ are the model parameters;\n",
        "+ $m$ denotes the number of output channels of the CNN;\n",
        "+ $\\delta$ denotes the width (kernel size) of the convolution;\n",
        "+ $\\sigma(\\cdot)$ denotes the standard sigmoid function;\n",
        "+ $\\max(\\cdot)$ presents the max pooling operation;\n",
        "+ ${\\rm ReLU}(\\cdot)$ denotes the ReLU activation function;\n",
        "+ $\\oplus$ presents a concatenation of vectors.\n",
        "\n",
        "Setting the hyperparameters $d=3, m=2, \\delta=2$, we initialize the model parameters as follows.\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{W} &= \\begin{pmatrix}\n",
        "-3 & -2 & -1 & -1 & -2 & -3 \\\\\n",
        "3 & 2 & 3 & 2 & 3 & 2\n",
        "\\end{pmatrix} \\\\\n",
        "\\boldsymbol{b} &= \\begin{pmatrix}\n",
        "-0.2 \\\\ 0.1\n",
        "\\end{pmatrix} \\\\\n",
        "\\boldsymbol{v} &= \\begin{pmatrix}\n",
        "-1 \\\\ 2\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Suppose that we give a negative ($y=0$) training instance with the sentence ($T = 5$),\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{X} &= \\begin{pmatrix}\n",
        "-0.3 & 0 & 0.1 & 0 & 0 \\\\\n",
        "-0.2 & -0.1 & 0 & 0.1 & 0 \\\\\n",
        "-0.1 & -0.2 & 0.1 & 0 & 0.1\n",
        "\\end{pmatrix} ,\n",
        "\\end{align}\n",
        "to the CNN model, and answer the following questions.\n",
        "\n",
        "**(1)** Find the value of the vector $\\boldsymbol{x}_{3:4}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要ライブラリのインポート\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[-3., -2., -1., -1., -2., -3.],\n",
              "        [ 3.,  2.,  3.,  2.,  3.,  2.]]),\n",
              " array([[-0.2],\n",
              "        [ 0.1]]),\n",
              " array([[-1.],\n",
              "        [ 2.]]),\n",
              " array([[-0.3,  0. ,  0.1,  0. ,  0. ],\n",
              "        [-0.2, -0.1,  0. ,  0.1,  0. ],\n",
              "        [-0.1, -0.2,  0.1,  0. ,  0.1]]),\n",
              " 3,\n",
              " 2,\n",
              " 2)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 各種変数の準備\n",
        "W = np.array([[-3.0, -2.0, -1.0, -1.0, -2.0, -3.0], [3.0, 2.0, 3.0, 2.0, 3.0, 2.0]])\n",
        "b = np.array([[-0.2], [0.1]])\n",
        "v = np.array([[-1.0], [2.0]])\n",
        "X = np.array([[-0.3, 0, 0.1, 0, 0], [-0.2, -0.1, 0, 0.1, 0], [-0.1, -0.2, 0.1, 0, 0.1]])\n",
        "W, b, v, X, d, m, delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "uec36GNAyNdz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.1, 0. , 0.1, 0. , 0.1, 0. ])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# x_{3:4}\n",
        "np.concatenate([X[:, 2], X[:, 3]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XCf-KoPyOfp"
      },
      "source": [
        "**(2)** Find the values of the hidden vectors $\\boldsymbol{c}_1, \\boldsymbol{c}_2, \\boldsymbol{c}_3, \\boldsymbol{c}_4$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "sgxmns3JyQgB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[2.],\n",
              "        [0.]]),\n",
              " array([[0.],\n",
              "        [0.]]),\n",
              " array([[0.],\n",
              "        [1.]]),\n",
              " array([[0. ],\n",
              "        [0.5]]))"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# コンテキストベクトルc_tの計算\n",
        "def c(t) :\n",
        "    val = (W @ np.concatenate([X[:, t], X[:, t+1]])).reshape(2, 1) + b\n",
        "    return np.maximum(0, val)\n",
        "\n",
        "c(0), c(1), c(2), c(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVFqVPWPyQ58"
      },
      "source": [
        "**(3)** Find the value of the vector $\\boldsymbol{s}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "Vt3UQqqAyTVj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2.],\n",
              "       [1.]])"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = np.max(np.hstack([c(0), c(1), c(2), c(3)]), axis=1)\n",
        "s.reshape(2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt26e_TSyT6W"
      },
      "source": [
        "**(4)** Find the value of $p$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "U4g6j4L3y635"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5000000000000002"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "p = sigmoid(v.T @ s)[0]\n",
        "p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUh6ZlAqyWRE"
      },
      "source": [
        "**(5)** Write the formula of the binary cross-entropy loss between the correct label $y$ and the probability estimate $p$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "QdCPWs4UyV9b"
      },
      "outputs": [],
      "source": [
        "# 注：p=0,1の時の処理は省略する\n",
        "def BCE(y, p):\n",
        "    return -(y * np.log(p) + (1-y) * np.log(1-p)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9WQToWfyYuj"
      },
      "source": [
        "**(6)** Compute the loss value by using the formula of (5) for the training instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "LsFTKdK3zpYC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6931471805599457"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = BCE(0, p)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b03t2amzpz5"
      },
      "source": [
        "**(7)** Compute the gradient of the loss function with respect to $\\boldsymbol{v}$ for the training instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "負例の場合の勾配は、$\\displaystyle\\frac{\\partial BCE(p)}{\\partial v}=\\frac{1}{1-p}\\cdot p(1-p)\\cdot s$と求まることから、以下のように計算できる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "Oa0xGZNgz7bY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1. ],\n",
              "       [0.5]])"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grad_v = 1/(1-p) * p * (1-p) * s\n",
        "grad_v.reshape(2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTt2o7hKz7zD"
      },
      "source": [
        "**(8)** Compute the gradients of the loss function with respect to $\\boldsymbol{W}$ for the training instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(7)と同様にして誤差逆伝播法で、勾配を計算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "0mz0yeWF0PIy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.15,  0.1 ,  0.05, -0.  ,  0.05,  0.1 ],\n",
              "       [ 0.1 ,  0.  ,  0.1 ,  0.  ,  0.1 ,  0.  ]])"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grad_W = 1/(1-p) * p * (1-p) * v * np.array([np.concatenate([X[:, 0], X[:, 0+1]]), np.concatenate([X[:, 2], X[:, 2+1]])])\n",
        "grad_W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPWwlW4F0P0i"
      },
      "source": [
        "## Question 2 (20 points)\n",
        "\n",
        "Give names of two datasets that can be used to evaluate the quality of word embeddings, and explain the datasets with the following perspectives.\n",
        "\n",
        "+ Brief explanation of the task for the evaluation.\n",
        "+ Statistics of the dataset (e.g., the number of instances)\n",
        "+ Measure(s) for evaluating the quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdNBHor63biK"
      },
      "source": [
        "### Natural Questions\n",
        "- いわゆる、**オープンドメインの質問応答** (Open-Domain Question Answering) と呼ばれるタスクの代表的なデータセットの1つである。与えられた質問文に対する回答を、WikipediaなどのWeb文書上から探し出して回答する。一般に、Retriever(検索器)モデルで回答の証拠となりそうな文書を抽出し、それをもとにGenerator(生成器)やReader(スパン抽出器)で最終的な回答を生成したり、文書中から抽出したりする。これらのあらゆる過程で単語埋め込みの品質は重要となる。\n",
        "- 学習データ：307,373個 / 検証データ：7,830 examples / テストデータ：7,842個\n",
        "- Natural Questionsを含む、一般的なQAは回答が短答式であるため、参照回答と予測回答との**完全一致**(Exact Match・EM)をベースに評価することが基本となっている。\n",
        "\n",
        "### QMSum\n",
        "- 比較的新しい**要約**データセットの1つであり、議事録形式であること、さらにクエリ指向型であることが特徴である。一般的な要約タスクは、ある文書全体の要約を作成することが目標であるが、クエリ指向型要約では、特定のクエリ(質問・お題)に沿った要約を作成することが求められる (例：「Aさんは今期の売上業績についてどのような意見を述べていますか？」など)。また、議事録は対話データであり、一般的な文書データよりも合計トークン数が多い、対話特有の構造や談話内容の理解が難しい、などの性質から、QMSumは多くの要約データセットの中でもタスクとしての難易度が高い部類に相当すると考えられる。\n",
        "- 学習データ：1,257個 / 検証データ：272個 / テストデータ：279個 (※クエリとソース文のペアでカウント)\n",
        "- データ数自体はかなり少ないが、1つ1つの会議の平均トークン長は9,069.8であり、非常に長い\n",
        "- QMSumを含む、一般的な要約タスクは、参照回答と予測回答との**ROUGE**スコアで評価することが基本となっている。これは両者間の語彙重複を評価する指標であり、**ROUGE-1・ROUGE-2・ROUGE-L・ROUGE-Lsum**などの種類がある。ただし、ROUGEスコアのみだと、実際の生成文の流暢性・事実への忠実性・情報量などの観点での評価ができないため、自動評価に加えて、**人間による評価**を合わせて実施することも多い。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYrvAyBW3TxD"
      },
      "source": [
        "## Question 3 (20 points)\n",
        "\n",
        "Explain two reasons why Transformers are superior to Recurrent Neural Network\n",
        "(RNN) in sequence-to-sequence tasks such as Machine Translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RNNでは時系列データの直前までの情報を1つの隠れベクトルとして埋め込むことになるが、これに埋め込むことのできる情報量は限界がある。また、モデルの構造上、遠いトークン間の情報はどうしても伝達に難がある。一方でTransformerの場合は、基本的に全てのトークン対についてAttentionメカニズムで情報を伝達できるため、遠いトークン間を含めて、より豊富な情報量を表現することが可能である。自然言語処理では、長距離の文脈を考慮しなければならないようなタスクも多いため、そのような場合には特に、TransformerはRNNに対する高い優位性を持つと考えられる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqfAdU713cYa"
      },
      "source": [
        "RNNはある時刻上での隠れベクトルを逐次的に計算していく必要があるが、GPUの並列化によって計算時間を短縮することが重要となる深層学習において、この性質は非常にネックである。一方でTransformerは少なくとも学習時であれば、全てのAttentionの計算は独立に行えるため、計算の並列化が行いやすい。従って計算の並列化の面でも、TransformerはRNNに対する優位性があるといえる。(ただし、TransformerのAttentionも入力長に対して$O(N^2)$の計算量・メモリが必要となるため、非常に長い入力を扱う際には工夫が必要となる。)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYxI86gA3dGs"
      },
      "source": [
        "## Question 4 (10 points)\n",
        "\n",
        "Implement the code for using a pre-trained **language** model. Show the code and its output as well as the following information:\n",
        "\n",
        "+ The detail of the pre-trained language model, for example,\n",
        "    + https://huggingface.co/EleutherAI/gpt-j-6B\n",
        "    + https://huggingface.co/rinna/japanese-gpt-1b\n",
        "    + https://huggingface.co/facebook/blenderbot-400M-distill\n",
        "+ The task addressed by the model (e.g., \"text generation\", \"summarization\", \"chatbot\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCaAqhtCGC2I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "aml2022_report.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
