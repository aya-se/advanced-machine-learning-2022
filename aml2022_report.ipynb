{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aya-se/advanced-machine-learning-2022/blob/main/aml2022_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z-I39fBi1MD"
      },
      "source": [
        "# Advanced Machine Learning (2022) Final Report Assignment\n",
        "\n",
        "Answer Questions 1 to 4 (either in Japanese or English). Submit a report in either PDF (.pdf) or JupyterNotebook (.ipynb) format.\n",
        "\n",
        "## Question 1 (50 points)\n",
        "\n",
        "Consider a convolutional neural network (CNN) that predicts a label $\\hat{y} \\in \\{0, 1\\}$ for a given sentence $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times T}$. Here, a sentence is represented by a matrix $\\boldsymbol{X} = (\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_T)$ consisting of a concatenation of $T$ word embeddings, $\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_T \\in \\mathbb{R}^d$, where $d$ is the size of word embeddings, and $T$ is the number of words in the sentence.\n",
        "\n",
        "These equations define the whole architecture of the CNN.\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y} &= \\begin{cases}\n",
        "1 & (0.5 < p) \\\\\n",
        "0 & (p \\leq 0.5)\n",
        "\\end{cases} \\\\\n",
        "p &= \\sigma(\\boldsymbol{v}^\\top \\boldsymbol{s}) \\\\\n",
        "\\boldsymbol{s} &= \\max(\\boldsymbol{c}_1, \\dots, \\boldsymbol{c}_{T-\\delta+1}) \\\\\n",
        "\\boldsymbol{c}_t &= {\\rm ReLU}(\\boldsymbol{W} \\boldsymbol{x}_{t:t+\\delta-1} + \\boldsymbol{b}) & (\\forall t \\in \\{1, \\dots, T-\\delta+1\\}) \\\\\n",
        "\\boldsymbol{x}_{t:t+\\delta-1} &= \\boldsymbol{x}_{t} \\oplus \\boldsymbol{x}_{t+1} \\oplus \\dots \\oplus \\boldsymbol{x}_{t+\\delta-1}\n",
        "\\end{align}\n",
        "\n",
        "Here:\n",
        "\n",
        "+ $\\boldsymbol{W} \\in \\mathbb{R}^{m \\times \\delta d}$, $\\boldsymbol{b} \\in \\mathbb{R}^m, \\boldsymbol{v} \\in \\mathbb{R}^m$ are the model parameters;\n",
        "+ $m$ denotes the number of output channels of the CNN;\n",
        "+ $\\delta$ denotes the width (kernel size) of the convolution;\n",
        "+ $\\sigma(\\cdot)$ denotes the standard sigmoid function;\n",
        "+ $\\max(\\cdot)$ presents the max pooling operation;\n",
        "+ ${\\rm ReLU}(\\cdot)$ denotes the ReLU activation function;\n",
        "+ $\\oplus$ presents a concatenation of vectors.\n",
        "\n",
        "Setting the hyperparameters $d=3, m=2, \\delta=2$, we initialize the model parameters as follows.\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{W} &= \\begin{pmatrix}\n",
        "-3 & -2 & -1 & -1 & -2 & -3 \\\\\n",
        "3 & 2 & 3 & 2 & 3 & 2\n",
        "\\end{pmatrix} \\\\\n",
        "\\boldsymbol{b} &= \\begin{pmatrix}\n",
        "-0.2 \\\\ 0.1\n",
        "\\end{pmatrix} \\\\\n",
        "\\boldsymbol{v} &= \\begin{pmatrix}\n",
        "-1 \\\\ 2\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Suppose that we give a negative ($y=0$) training instance with the sentence ($T = 5$),\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{X} &= \\begin{pmatrix}\n",
        "-0.3 & 0 & 0.1 & 0 & 0 \\\\\n",
        "-0.2 & -0.1 & 0 & 0.1 & 0 \\\\\n",
        "-0.1 & -0.2 & 0.1 & 0 & 0.1\n",
        "\\end{pmatrix} ,\n",
        "\\end{align}\n",
        "to the CNN model, and answer the following questions.\n",
        "\n",
        "**(1)** Find the value of the vector $\\boldsymbol{x}_{3:4}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 必要ライブラリのインポート\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[-3., -2., -1., -1., -2., -3.],\n",
              "        [ 3.,  2.,  3.,  2.,  3.,  2.]]),\n",
              " array([[-0.2],\n",
              "        [ 0.1]]),\n",
              " array([[-1.],\n",
              "        [ 2.]]),\n",
              " array([[-0.3,  0. ,  0.1,  0. ,  0. ],\n",
              "        [-0.2, -0.1,  0. ,  0.1,  0. ],\n",
              "        [-0.1, -0.2,  0.1,  0. ,  0.1]]))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 各種変数の準備\n",
        "W = np.array([[-3.0, -2.0, -1.0, -1.0, -2.0, -3.0], [3.0, 2.0, 3.0, 2.0, 3.0, 2.0]])\n",
        "b = np.array([[-0.2], [0.1]])\n",
        "v = np.array([[-1.0], [2.0]])\n",
        "X = np.array([[-0.3, 0, 0.1, 0, 0], [-0.2, -0.1, 0, 0.1, 0], [-0.1, -0.2, 0.1, 0, 0.1]])\n",
        "W, b, v, X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uec36GNAyNdz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.1, 0. , 0.1, 0. , 0.1, 0. ])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# x_{3:4}\n",
        "np.concatenate([X[:, 2], X[:, 3]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XCf-KoPyOfp"
      },
      "source": [
        "**(2)** Find the values of the hidden vectors $\\boldsymbol{c}_1, \\boldsymbol{c}_2, \\boldsymbol{c}_3, \\boldsymbol{c}_4$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sgxmns3JyQgB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[2.],\n",
              "        [0.]]),\n",
              " array([[0.],\n",
              "        [0.]]),\n",
              " array([[0.],\n",
              "        [1.]]),\n",
              " array([[0. ],\n",
              "        [0.5]]))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# コンテキストベクトルc_tの計算\n",
        "def c(t) :\n",
        "    val = (W @ np.concatenate([X[:, t], X[:, t+1]])).reshape(2, 1) + b\n",
        "    return np.maximum(0, val)\n",
        "\n",
        "c(0), c(1), c(2), c(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVFqVPWPyQ58"
      },
      "source": [
        "**(3)** Find the value of the vector $\\boldsymbol{s}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vt3UQqqAyTVj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2.],\n",
              "       [1.]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = np.max(np.hstack([c(0), c(1), c(2), c(3)]), axis=1)\n",
        "s.reshape(2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt26e_TSyT6W"
      },
      "source": [
        "**(4)** Find the value of $p$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U4g6j4L3y635"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5000000000000002"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "p = sigmoid(v.T @ s)[0]\n",
        "p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUh6ZlAqyWRE"
      },
      "source": [
        "**(5)** Write the formula of the binary cross-entropy loss between the correct label $y$ and the probability estimate $p$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QdCPWs4UyV9b"
      },
      "outputs": [],
      "source": [
        "# 注：p=0,1の時の処理は省略する\n",
        "def BCE(y, p):\n",
        "    return -(y * np.log(p) + (1-y) * np.log(1-p)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9WQToWfyYuj"
      },
      "source": [
        "**(6)** Compute the loss value by using the formula of (5) for the training instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LsFTKdK3zpYC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6931471805599457"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = BCE(0, p)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b03t2amzpz5"
      },
      "source": [
        "**(7)** Compute the gradient of the loss function with respect to $\\boldsymbol{v}$ for the training instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "負例の場合の勾配は、$\\displaystyle\\frac{\\partial BCE(p)}{\\partial v}=\\frac{1}{1-p}\\cdot p(1-p)\\cdot s$と求まることから、以下のように計算できる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Oa0xGZNgz7bY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1. ],\n",
              "       [0.5]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grad_v = 1/(1-p) * p * (1-p) * s\n",
        "grad_v.reshape(2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTt2o7hKz7zD"
      },
      "source": [
        "**(8)** Compute the gradients of the loss function with respect to $\\boldsymbol{W}$ for the training instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(7)と同様にして誤差逆伝播法で、勾配を計算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0mz0yeWF0PIy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.15,  0.1 ,  0.05, -0.  ,  0.05,  0.1 ],\n",
              "       [ 0.1 ,  0.  ,  0.1 ,  0.  ,  0.1 ,  0.  ]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grad_W = 1/(1-p) * p * (1-p) * v * np.array([np.concatenate([X[:, 0], X[:, 0+1]]), np.concatenate([X[:, 2], X[:, 2+1]])])\n",
        "grad_W"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPWwlW4F0P0i"
      },
      "source": [
        "## Question 2 (20 points)\n",
        "\n",
        "Give names of two datasets that can be used to evaluate the quality of word embeddings, and explain the datasets with the following perspectives.\n",
        "\n",
        "+ Brief explanation of the task for the evaluation.\n",
        "+ Statistics of the dataset (e.g., the number of instances)\n",
        "+ Measure(s) for evaluating the quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdNBHor63biK"
      },
      "source": [
        "### Natural Questions\n",
        "- いわゆる、**オープンドメインの質問応答** (Open-Domain Question Answering) と呼ばれるタスクの代表的なデータセットの1つである。与えられた質問文に対する回答を、WikipediaなどのWeb文書上から探し出して回答する。一般に、Retriever(検索器)モデルで回答の証拠となりそうな文書を抽出し、それをもとにGenerator(生成器)やReader(スパン抽出器)で最終的な回答を生成したり、文書中から抽出したりする。これらのあらゆる過程で単語埋め込みの品質は重要となる。\n",
        "- 学習データ：307,373個 / 検証データ：7,830 examples / テストデータ：7,842個\n",
        "- Natural Questionsを含む、一般的なQAは回答が短答式であるため、参照回答と予測回答との**完全一致**(Exact Match・EM)をベースに評価することが基本となっている。\n",
        "\n",
        "### QMSum\n",
        "- 比較的新しい**要約**データセットの1つであり、議事録形式であること、さらにクエリ指向型であることが特徴である。一般的な要約タスクは、ある文書全体の要約を作成することが目標であるが、クエリ指向型要約では、特定のクエリ(質問・お題)に沿った要約を作成することが求められる (例：「Aさんは今期の売上業績についてどのような意見を述べていますか？」など)。また、議事録は対話データであり、一般的な文書データよりも合計トークン数が多い、対話特有の構造や談話内容の理解が難しい、などの性質から、QMSumは多くの要約データセットの中でもタスクとしての難易度が高い部類に相当すると考えられる。\n",
        "- 学習データ：1,257個 / 検証データ：272個 / テストデータ：279個 (※クエリとソース文のペアでカウント)\n",
        "- データ数自体はかなり少ないが、1つ1つの会議の平均トークン長は9,069.8であり、非常に長い\n",
        "- QMSumを含む、一般的な要約タスクは、参照回答と予測回答との**ROUGE**スコアで評価することが基本となっている。これは両者間の語彙重複を評価する指標であり、**ROUGE-1・ROUGE-2・ROUGE-L・ROUGE-Lsum**などの種類がある。ただし、ROUGEスコアのみだと、実際の生成文の流暢性・事実への忠実性・情報量などの観点での評価ができないため、自動評価に加えて、**人間による評価**を合わせて実施することも多い。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYrvAyBW3TxD"
      },
      "source": [
        "## Question 3 (20 points)\n",
        "\n",
        "Explain two reasons why Transformers are superior to Recurrent Neural Network\n",
        "(RNN) in sequence-to-sequence tasks such as Machine Translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- RNNでは時系列データの直前までの情報を1つの隠れベクトルとして埋め込むことになるが、これに埋め込むことのできる情報量は限界がある。また、モデルの構造上、遠いトークン間の情報はどうしても伝達に難がある。一方でTransformerの場合は、基本的に全てのトークン対についてAttentionメカニズムで情報を伝達できるため、遠いトークン間を含めて、より豊富な情報量を表現することが可能である。自然言語処理では、長距離の文脈を考慮しなければならないようなタスクも多いため、そのような場合には特に、TransformerはRNNに対する高い優位性を持つと考えられる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqfAdU713cYa"
      },
      "source": [
        "- RNNはある時刻上での隠れベクトルを逐次的に計算していく必要があるが、GPUの並列化によって計算時間を短縮することが重要となる深層学習において、この性質は非常にネックである。一方でTransformerは少なくとも学習時であれば、全てのAttentionの計算は独立に行えるため、計算の並列化が行いやすい。従って計算の並列化の面でも、TransformerはRNNに対する優位性があるといえる。(ただし、TransformerのAttentionも入力長に対して$O(N^2)$の計算量・メモリが必要となるため、非常に長い入力を扱う際には工夫が必要となる。)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYxI86gA3dGs"
      },
      "source": [
        "## Question 4 (10 points)\n",
        "\n",
        "Implement the code for using a pre-trained **language** model. Show the code and its output as well as the following information:\n",
        "\n",
        "+ The detail of the pre-trained language model, for example,\n",
        "    + https://huggingface.co/EleutherAI/gpt-j-6B\n",
        "    + https://huggingface.co/rinna/japanese-gpt-1b\n",
        "    + https://huggingface.co/facebook/blenderbot-400M-distill\n",
        "+ The task addressed by the model (e.g., \"text generation\", \"summarization\", \"chatbot\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HuggingFaceのBARTに岡崎研究室の紹介文を流し込んだだけ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab_file vocab.json\n",
            "merges_file merges.txt\n",
            "tokenizer_file tokenizer.json\n",
            "added_tokens_file added_tokens.json\n",
            "special_tokens_map_file special_tokens_map.json\n",
            "tokenizer_config_file tokenizer_config.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': 'The students joined the laboratory to acquire skills through research activities and make a confident start as a member of society in the future. The laboratory has diverse people with various hobbies, skills, and thoughts. They respect the individuality of each member, learn from others, and have fun together.'}]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# HuggingFaceから要約用にファインチューニングされたBARTの生成モデルを使用\n",
        "# https://huggingface.co/philschmid/bart-large-cnn-samsum\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"philschmid/bart-large-cnn-samsum\")\n",
        "\n",
        "ARTICLE_TO_SUMMARIZE = \"We hope the students joined the laboratory to acquire skills through research activities and make a confident start as a member of society in the future. \\\n",
        "                        Once you start working in the community, you are not only required to solve the problem of your clients or others, \\\n",
        "                        but also to spot or suggest a potential issue which even the client doesn’t notice. When you write a bachelor/master thesis, \\\n",
        "                        you also need to explore the unsolved problems by yourself, address them by all possible means to search for a solution, and give your findings back to the society. \\\n",
        "                        Through your research, you can acquire abilities to establish your specialty as if building your castle, to complete theories/works, and to appeal to others, \\\n",
        "                        which would be useful anywhere in the society. There is always someone who has been conducting a related study in the world. \\\n",
        "                        Whenever you advance your research, you will face challenges from the world. For this reason, even when you write a bachelor/master thesis, \\\n",
        "                        you should set the research topic bearing in mind the need to present the findings of your research in international conferences or journals. \\\n",
        "                        The laboratory has diverse people with various hobbies, skills, and thoughts; we respect the individuality of each member, learn from others, and have fun together. \\\n",
        "                        We hope that your experiences in the laboratory will be invaluable for the rest of your lives.\"\n",
        "\n",
        "summarizer(ARTICLE_TO_SUMMARIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (参考)日本語訳\n",
        "#### 原文\n",
        "研究室には、研究活動を通じてスキルを身につけ、将来、社会人として自信を持ってスタートできるような人材を求めて入室してもらいました。社会で仕事をするようになると、クライアントなどの問題を解決するだけでなく、クライアントも気づいていない潜在的な問題を発見し、提案することが求められます。また、学士・修士論文では、未解決の問題を自ら掘り起こし、あらゆる手段を使って解決策を模索し、その成果を社会に還元することが求められます。研究を通して、自分の城を築くように専門性を確立し、理論や作品を完成させ、社会にアピールする能力を身につけることができ、それは社会のどこでも通用するものです。\n",
        "\n",
        "世の中には、必ず関連する研究をしている人がいます。研究を進めると、必ず世の中の課題に直面する。ですから、学士・修士論文を書く場合でも、研究成果を国際会議や学術誌で発表することを念頭に置いて、研究テーマを設定する必要があります。\n",
        "\n",
        "研究室には、様々な趣味、技術、考えを持った人が集まっており、それぞれの個性を尊重し、他者から学び、共に楽しむことができます。研究室での経験が、皆さんの生涯に渡ってかけがえのないものになることを願っています。\n",
        "\n",
        "#### 要約結果\n",
        "研究活動を通じて技術を身につけ、将来、社会人として自信を持ってスタートを切るために、研究室に入りました。研究室には、さまざまな趣味や特技、考えを持った人が集まっています。それぞれの個性を尊重し、他者から学び、共に楽しみます。\n",
        "\n",
        "---\n",
        "\n",
        "ちょっとニュアンスが怪しいが、まぁだいだい妥当な要約にはなっている。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "aml2022_report.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
